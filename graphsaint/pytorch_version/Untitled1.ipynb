{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "os.chdir('/home/dqw_zyf/MDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, <pyximport.pyximport.PyxImporter at 0x7fe924698ac0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import    pyximport\n",
    "\n",
    "pyximport.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: Not a git repository (or any parent up to mount point /home)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: Not a git repository (or any parent up to mount point /home)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    }
   ],
   "source": [
    "# %load train.py\n",
    "from graphsaint.globals import *\n",
    "from graphsaint.pytorch_version.models import GraphSAINT\n",
    "#from graphsaint.pytorch_version.minibatch import Minibatch\n",
    "from graphsaint.utils import *\n",
    "from graphsaint.metric import *\n",
    "from graphsaint.pytorch_version.utils import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_full_batch(model, minibatch, mode='val'):\n",
    "    \"\"\"\n",
    "    Full batch evaluation: for validation and test sets only.\n",
    "        When calculating the F1 score, we will mask the relevant root nodes\n",
    "        (e.g., those belonging to the val / test sets).\n",
    "    \"\"\"\n",
    "    loss,preds,labels = model.eval_step(*minibatch.one_batch(mode=mode))  #模型评估\n",
    "    #print(labels.shape, preds.shape)\n",
    "    #print('Labels \\n', labels)\n",
    "    #print('Preds \\n', preds)\n",
    "    if mode == 'val':\n",
    "        printf('Val: loss = {:.4f}'.format(loss), style = 'red')\n",
    "        node_target = [minibatch.node_val]   \n",
    "    elif mode == 'Test':\n",
    "        printf('Test: loss = {:.4f}'.format(loss), style = 'red')\n",
    "        node_target = [minibatch.node_test]\n",
    "    else:\n",
    "        print('Validation & Test: ')\n",
    "        assert mode == 'valtest'\n",
    "        node_target = [minibatch.node_val, minibatch.node_test]\n",
    "        \n",
    "   \n",
    "    accuracy, precision, recall, f1, roc_auc, aupr, pos_acc, neg_acc = [], [], [], [], [], [], [], []\n",
    "    for n in node_target:\n",
    "        #print(labels[n].shape, preds[n].shape)\n",
    "        #print(labels[n,1], preds[n,1])\n",
    "        \"\"\" metrics(y_true, y_prob, is_sigmoid, isprint = True)\n",
    "            return (y_true, y_pred, y_prob), (accuracy, precision, recall, f1, roc_auc, aupr, pos_acc, neg_acc)\n",
    "        \"\"\"\n",
    "        ys, performances = metrics(to_numpy(labels[n, 1]), to_numpy(preds[n, 1]), model.sigmoid_loss)\n",
    "        \n",
    "        accuracy.append(performances[0])\n",
    "        precision.append(performances[1])\n",
    "        recall.append(performances[2])\n",
    "        f1.append(performances[3])\n",
    "        roc_auc.append(performances[4])\n",
    "        aupr.append(performances[5])\n",
    "        pos_acc.append(performances[6])\n",
    "        neg_acc.append(performances[7])\n",
    "    \n",
    "    pos_acc = pos_acc[0] if len(pos_acc) == 1 else pos_acc\n",
    "    neg_acc = neg_acc[0] if len(neg_acc) == 1 else neg_acc\n",
    "    roc_auc = roc_auc[0] if len(roc_auc) == 1 else roc_auc\n",
    "    aupr = aupr[0] if len(aupr) == 1 else aupr\n",
    "    f1 = f1[0] if len(f1) == 1 else f1\n",
    "    recall = recall[0] if len(recall) == 1 else recall\n",
    "    precision = precision[0] if len(precision) == 1 else precision\n",
    "    accuracy = accuracy[0] if len(accuracy) == 1 else accuracy\n",
    "    \n",
    "    return loss, ys, (accuracy, precision, recall, f1, roc_auc, aupr, pos_acc, neg_acc) # (y_true, y_pred, y_prob), (accuracy, precision, recall, f1, roc_auc, aupr, pos_acc, neg_acc)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def prepare(train_data,train_params,arch_gcn):\n",
    "    \"\"\"\n",
    "    Prepare some data structure and initialize model / minibatch handler before\n",
    "    the actual iterative training taking place.\n",
    "    \"\"\"\n",
    "    adj_full, adj_train, feat_full, class_arr,role = train_data\n",
    "    adj_full = adj_full.astype(np.int32)\n",
    "    adj_train = adj_train.astype(np.int32)\n",
    "    adj_full_norm = adj_norm(adj_full)\n",
    "    num_classes = class_arr.shape[1]\n",
    "\n",
    "    minibatch = Minibatch(adj_full_norm, adj_train, role, train_params)\n",
    "    model = GraphSAINT(num_classes, arch_gcn, train_params, feat_full, class_arr)\n",
    "    printf(\"TOTAL NUM OF PARAMS = {}\".format(sum(p.numel() for p in model.parameters())), style=\"yellow\")\n",
    "    minibatch_eval=Minibatch(adj_full_norm, adj_train, role, train_params, cpu_eval=True)\n",
    "    model_eval=GraphSAINT(num_classes, arch_gcn, train_params, feat_full, class_arr, cpu_eval=True)\n",
    "    if args_global.gpu >= 0:\n",
    "        model = model.cuda()\n",
    "    return model, minibatch, minibatch_eval, model_eval\n",
    "\n",
    "\n",
    "def train(train_phases, model, minibatch, minibatch_eval, model_eval, eval_val_every):\n",
    "    if not args_global.cpu_eval:\n",
    "        minibatch_eval=minibatch\n",
    "    epoch_ph_start = 0\n",
    "    \n",
    "    auc_best, ep_best = 0, -1\n",
    "    time_train = 0\n",
    "    dir_saver = '{}/pytorch_models'.format(args_global.dir_log)\n",
    "    path_saver = '{}/pytorch_models/mirna_disease_saved_model_{}.pkl'.format(args_global.dir_log, timestamp)\n",
    "    print('dir_saver: ', dir_saver)\n",
    "    print('path_saver: ', path_saver)\n",
    "    for ip, phase in enumerate(train_phases):\n",
    "        printf('START PHASE {:4d}'.format(ip),style='underline')\n",
    "        minibatch.set_sampler(phase)\n",
    "        num_batches = minibatch.num_training_batches()\n",
    "        for e in range(epoch_ph_start, int(phase['end'])):\n",
    "            printf('Epoch {:4d}'.format(e),style='bold')\n",
    "            minibatch.shuffle()\n",
    "            \n",
    "            l_loss_tr, lr_accuracy_tr, lr_precision_tr, lr_recall_tr, lr_f1_tr, lr_roc_auc_tr, lr_aupr_tr, lr_pos_acc_tr, lr_neg_acc_tr = [], [], [], [], [], [], [], [], []\n",
    "            \n",
    "            time_train_ep = 0\n",
    "            while not minibatch.end():\n",
    "                t1 = time.time()\n",
    "                loss_train,preds_train,labels_train = model.train_step(*minibatch.one_batch(mode='train'))\n",
    "                time_train_ep += time.time() - t1\n",
    "                if not minibatch.batch_num % args_global.eval_train_every:\n",
    "                    \n",
    "                    ys_train, metrics_train = metrics(to_numpy(labels_train[:, 1]),to_numpy(preds_train[:, 1]),model.sigmoid_loss, isprint = True)\n",
    "                    l_loss_tr.append(loss_train)\n",
    "                    lr_accuracy_tr.append(metrics_train[0])\n",
    "                    lr_precision_tr.append(metrics_train[1])\n",
    "                    lr_recall_tr.append(metrics_train[2])\n",
    "                    lr_f1_tr.append(metrics_train[3])\n",
    "                    lr_roc_auc_tr.append(metrics_train[4])\n",
    "                    lr_aupr_tr.append(metrics_train[5])\n",
    "                    lr_pos_acc_tr.append(metrics_train[6])\n",
    "                    lr_neg_acc_tr.append(metrics_train[7])\n",
    "                    \n",
    "            if (e+1)%eval_val_every == 0:\n",
    "                if args_global.cpu_eval:\n",
    "                    torch.save(model.state_dict(),'tmp.pkl')\n",
    "                    model_eval.load_state_dict(torch.load('tmp.pkl',map_location=lambda storage, loc: storage))\n",
    "                else:\n",
    "                    model_eval = model\n",
    "                    \n",
    "                printf('Train (Ep avg): loss = {:.4f} | Time = {:.4f}sec'.format(f_mean(l_loss_tr), time_train_ep), style = 'yellow')\n",
    "                printf('acc={:.4f}|precision={:.4f}|recall={:.4f}|f1={:.4f}|auc={:.4f}|aupr={:.4f}|pos_acc={:.4f}|neg_acc={:.4f}'.format(\n",
    "f_mean(lr_accuracy_tr), f_mean(lr_precision_tr), f_mean(lr_recall_tr), f_mean(lr_f1_tr), f_mean(lr_roc_auc_tr), f_mean(lr_aupr_tr), f_mean(lr_pos_acc_tr), f_mean(lr_neg_acc_tr)), \n",
    "style = 'yellow')\n",
    "                              \n",
    "                loss_val, ys_val, metrics_val = evaluate_full_batch(model_eval, minibatch_eval, mode='val')\n",
    "\n",
    "                auc_val = metrics_val[4]\n",
    "                if auc_val > auc_best:\n",
    "                    auc_best, ep_best = auc_val, e\n",
    "                    if not os.path.exists(dir_saver):\n",
    "                        os.makedirs(dir_saver)\n",
    "                    printf('  Saving model ...', style='yellow')\n",
    "                    torch.save(model.state_dict(), path_saver)\n",
    "            time_train += time_train_ep\n",
    "        epoch_ph_start = int(phase['end'])\n",
    "    printf(\"Optimization Finished!\", style=\"yellow\")\n",
    "    if ep_best >= 0:\n",
    "        if args_global.cpu_eval:\n",
    "            model_eval.load_state_dict(torch.load(path_saver, map_location=lambda storage, loc: storage))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(path_saver))\n",
    "            model_eval=model\n",
    "        printf('  Restoring model ...', style='yellow')\n",
    "        \n",
    "    printf('Best Epoch = ' + str(ep_best), style = 'red')\n",
    "    loss_test, ys_test, metrics_test = evaluate_full_batch(model_eval, minibatch_eval, mode='val')\n",
    "    \n",
    "    printf(\"Total training time: {:6.2f} sec\".format(time_train), style='red')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    log_dir(args_global.train_config, args_global.data_prefix, git_branch, git_rev, timestamp)\n",
    "    train_params, train_phases, train_data, arch_gcn = parse_n_prepare(args_global)\n",
    "    if 'eval_val_every' not in train_params:\n",
    "        train_params['eval_val_every'] = EVAL_VAL_EVERY_EP\n",
    "    model, minibatch, minibatch_eval, model_eval = prepare(train_data, train_params, arch_gcn)\n",
    "    train(train_phases, model, minibatch, minibatch_eval, model_eval, train_params['eval_val_every'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Minibatch:\n",
    "    \"\"\"\n",
    "    Provides minibatches for the trainer or evaluator. This class is responsible for\n",
    "    calling the proper graph sampler and estimating normalization coefficients.\n",
    "    \"\"\"\n",
    "    def __init__(self, adj_full_norm, adj_train, role, train_params, cpu_eval=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            adj_full_norm       scipy CSR, adj matrix for the full graph (row-normalized)\n",
    "            adj_train           scipy CSR, adj matrix for the traing graph. Since we are\n",
    "                                under transductive setting, for any edge in this adj,\n",
    "                                both end points must be training nodes.\n",
    "            role                dict, key 'tr' -> list of training node IDs;\n",
    "                                      key 'va' -> list of validation node IDs;\n",
    "                                      key 'te' -> list of test node IDs.\n",
    "            train_params        dict, additional parameters related to training. e.g.,\n",
    "                                how many subgraphs we want to get to estimate the norm\n",
    "                                coefficients.\n",
    "            cpu_eval            bool, whether or not we want to run full-batch evaluation\n",
    "                                on the CPU.\n",
    "\n",
    "        Outputs:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.use_cuda = (args_global.gpu >= 0)\n",
    "        if cpu_eval:\n",
    "            self.use_cuda=False\n",
    "\n",
    "        self.node_train = np.array(role['tr'])\n",
    "        self.node_val = np.array(role['va'])\n",
    "        self.node_test = np.array(role['te'])\n",
    "\n",
    "        self.adj_full_norm = _coo_scipy2torch(adj_full_norm.tocoo())\n",
    "        self.adj_train = adj_train\n",
    "        # -----------------------\n",
    "        # sanity check (optional)\n",
    "        # -----------------------\n",
    "        #for role_set in [self.node_val, self.node_test]:\n",
    "        #    for v in role_set:\n",
    "        #        assert self.adj_train.indptr[v+1] == self.adj_train.indptr[v]\n",
    "        #_adj_train_T = sp.csr_matrix.tocsc(self.adj_train)\n",
    "        #assert np.abs(_adj_train_T.indices - self.adj_train.indices).sum() == 0\n",
    "        #assert np.abs(_adj_train_T.indptr - self.adj_train.indptr).sum() == 0\n",
    "        #_adj_full_T = sp.csr_matrix.tocsc(adj_full_norm)\n",
    "        #assert np.abs(_adj_full_T.indices - adj_full_norm.indices).sum() == 0\n",
    "        #assert np.abs(_adj_full_T.indptr - adj_full_norm.indptr).sum() == 0\n",
    "        #printf(\"SANITY CHECK PASSED\", style=\"yellow\")\n",
    "        if self.use_cuda:\n",
    "            # now i put everything on GPU. Ideally, full graph adj/feat\n",
    "            # should be optionally placed on CPU\n",
    "            self.adj_full_norm = self.adj_full_norm.cuda()\n",
    "\n",
    "        # below: book-keeping for mini-batch\n",
    "        self.node_subgraph = None\n",
    "        self.batch_num = -1\n",
    "\n",
    "        self.method_sample = None\n",
    "        self.subgraphs_remaining_indptr = []\n",
    "        self.subgraphs_remaining_indices = []\n",
    "        self.subgraphs_remaining_data = []\n",
    "        self.subgraphs_remaining_nodes = []\n",
    "        self.subgraphs_remaining_edge_index = []\n",
    "\n",
    "        self.norm_loss_train = np.zeros(self.adj_train.shape[0])\n",
    "        # norm_loss_test is used in full batch evaluation (without sampling).\n",
    "        # so neighbor features are simply averaged.\n",
    "        self.norm_loss_test = np.zeros(self.adj_full_norm.shape[0])\n",
    "        _denom = len(self.node_train) + len(self.node_val) +  len(self.node_test)\n",
    "        self.norm_loss_test[self.node_train] = 1. / _denom\n",
    "        self.norm_loss_test[self.node_val] = 1. / _denom\n",
    "        self.norm_loss_test[self.node_test] = 1. / _denom\n",
    "        self.norm_loss_test = torch.from_numpy(self.norm_loss_test.astype(np.float32))\n",
    "        if self.use_cuda:\n",
    "            self.norm_loss_test = self.norm_loss_test.cuda()\n",
    "        self.norm_aggr_train = np.zeros(self.adj_train.size)\n",
    "\n",
    "        self.sample_coverage = train_params['sample_coverage']\n",
    "        self.deg_train = np.array(self.adj_train.sum(1)).flatten()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  def set_sampler(self, train_phases):\n",
    "        \"\"\"\n",
    "        Pick the proper graph sampler. Run the warm-up phase to estimate\n",
    "        loss / aggregation normalization coefficients.\n",
    "\n",
    "        Inputs:\n",
    "            train_phases       dict, config / params for the graph sampler\n",
    "\n",
    "        Outputs:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.subgraphs_remaining_indptr = []\n",
    "        self.subgraphs_remaining_indices = []\n",
    "        self.subgraphs_remaining_data = []\n",
    "        self.subgraphs_remaining_nodes = []\n",
    "        self.subgraphs_remaining_edge_index = []\n",
    "        self.method_sample = train_phases['sampler']\n",
    "        if self.method_sample == 'mrw':\n",
    "            if 'deg_clip' in train_phases:\n",
    "                _deg_clip = int(train_phases['deg_clip'])\n",
    "            else:\n",
    "                _deg_clip = 100000      # setting this to a large number so essentially there is no clipping in probability\n",
    "            self.size_subg_budget = train_phases['size_subgraph']\n",
    "            self.graph_sampler = mrw_sampling(\n",
    "                self.adj_train,\n",
    "                self.node_train,\n",
    "                self.size_subg_budget,\n",
    "                train_phases['size_frontier'],\n",
    "                _deg_clip,\n",
    "            )\n",
    "        elif self.method_sample == 'rw':\n",
    "            self.size_subg_budget = train_phases['num_root'] * train_phases['depth']\n",
    "            self.graph_sampler = rw_sampling(\n",
    "                self.adj_train,\n",
    "                self.node_train,\n",
    "                self.size_subg_budget,\n",
    "                int(train_phases['num_root']),\n",
    "                int(train_phases['depth']),\n",
    "            )\n",
    "        elif self.method_sample == 'edge':\n",
    "            self.size_subg_budget = train_phases['size_subg_edge'] * 2\n",
    "            self.graph_sampler = edge_sampling(\n",
    "                self.adj_train,\n",
    "                self.node_train,\n",
    "                train_phases['size_subg_edge'],\n",
    "            )\n",
    "        elif self.method_sample == 'node':\n",
    "            self.size_subg_budget = train_phases['size_subgraph']\n",
    "            self.graph_sampler = node_sampling(\n",
    "                self.adj_train,\n",
    "                self.node_train,\n",
    "                self.size_subg_budget,\n",
    "            )\n",
    "        elif self.method_sample == 'full_batch':\n",
    "            self.size_subg_budget = self.node_train.size\n",
    "            self.graph_sampler = full_batch_sampling(\n",
    "                self.adj_train,\n",
    "                self.node_train,\n",
    "                self.size_subg_budget,\n",
    "            )\n",
    "        elif self.method_sample == \"vanilla_node_python\":\n",
    "            self.size_subg_budget = train_phases[\"size_subgraph\"]\n",
    "            self.graph_sampler = NodeSamplingVanillaPython(\n",
    "                self.adj_train,\n",
    "                self.node_train,\n",
    "                self.size_subg_budget,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.norm_loss_train = np.zeros(self.adj_train.shape[0])\n",
    "        self.norm_aggr_train = np.zeros(self.adj_train.size).astype(np.float32)\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # BELOW: estimation of loss / aggregation normalization factors\n",
    "        # -------------------------------------------------------------\n",
    "        # For some special sampler, no need to estimate norm factors, we can calculate\n",
    "        # the node / edge probabilities directly.\n",
    "        # However, for integrity of the framework, we follow the same procedure\n",
    "        # for all samplers:\n",
    "        #   1. sample enough number of subgraphs\n",
    "        #   2. update the counter for each node / edge in the training graph\n",
    "        #   3. estimate norm factor alpha and lambda\n",
    "        tot_sampled_nodes = 0\n",
    "        while True:\n",
    "            self.par_graph_sample('train')\n",
    "            tot_sampled_nodes = sum([len(n) for n in self.subgraphs_remaining_nodes])\n",
    "            if tot_sampled_nodes > self.sample_coverage * self.node_train.size:\n",
    "                break\n",
    "        print()\n",
    "        num_subg = len(self.subgraphs_remaining_nodes)\n",
    "        for i in range(num_subg):\n",
    "            self.norm_aggr_train[self.subgraphs_remaining_edge_index[i]] += 1\n",
    "            self.norm_loss_train[self.subgraphs_remaining_nodes[i]] += 1\n",
    "        assert self.norm_loss_train[self.node_val].sum() + self.norm_loss_train[self.node_test].sum() == 0\n",
    "        for v in range(self.adj_train.shape[0]):\n",
    "            i_s = self.adj_train.indptr[v]\n",
    "            i_e = self.adj_train.indptr[v + 1]\n",
    "            val = np.clip(self.norm_loss_train[v] / self.norm_aggr_train[i_s : i_e], 0, 1e4)\n",
    "            val[np.isnan(val)] = 0.1\n",
    "            self.norm_aggr_train[i_s : i_e] = val\n",
    "        self.norm_loss_train[np.where(self.norm_loss_train==0)[0]] = 0.1\n",
    "        self.norm_loss_train[self.node_val] = 0\n",
    "        self.norm_loss_train[self.node_test] = 0\n",
    "        self.norm_loss_train[self.node_train] = num_subg / self.norm_loss_train[self.node_train] / self.node_train.size\n",
    "        self.norm_loss_train = torch.from_numpy(self.norm_loss_train.astype(np.float32))\n",
    "        if self.use_cuda:\n",
    "            self.norm_loss_train = self.norm_loss_train.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    def par_graph_sample(self,phase):\n",
    "        \"\"\"\n",
    "        Perform graph sampling in parallel. A wrapper function for graph_samplers.py\n",
    "        \"\"\"\n",
    "        t0 = time.time()\n",
    "        _indptr, _indices, _data, _v, _edge_index = self.graph_sampler.par_sample(phase)\n",
    "        t1 = time.time()\n",
    "        print('sampling 200 subgraphs:   time = {:.3f} sec'.format(t1 - t0), end=\"\\r\")\n",
    "        self.subgraphs_remaining_indptr.extend(_indptr)\n",
    "        self.subgraphs_remaining_indices.extend(_indices)\n",
    "        self.subgraphs_remaining_data.extend(_data)\n",
    "        self.subgraphs_remaining_nodes.extend(_v)\n",
    "        self.subgraphs_remaining_edge_index.extend(_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    def one_batch(self, mode='train'):\n",
    "        \"\"\"\n",
    "        Generate one minibatch for trainer. In the 'train' mode, one minibatch corresponds\n",
    "        to one subgraph of the training graph. In the 'val' or 'test' mode, one batch\n",
    "        corresponds to the full graph (i.e., full-batch rather than minibatch evaluation\n",
    "        for validation / test sets).\n",
    "\n",
    "        Inputs:\n",
    "            mode                str, can be 'train', 'val', 'test' or 'valtest'\n",
    "\n",
    "        Outputs:\n",
    "            node_subgraph       np array, IDs of the subgraph / full graph nodes\n",
    "            adj                 scipy CSR, adj matrix of the subgraph / full graph\n",
    "            norm_loss           np array, loss normalization coefficients. In 'val' or\n",
    "                                'test' modes, we don't need to normalize, and so the values\n",
    "                                in this array are all 1.\n",
    "        \"\"\"\n",
    "        if mode in ['val','test','valtest']:\n",
    "            self.node_subgraph = np.arange(self.adj_full_norm.shape[0])\n",
    "            adj = self.adj_full_norm\n",
    "        else:\n",
    "            assert mode == 'train'\n",
    "            if len(self.subgraphs_remaining_nodes) == 0:\n",
    "                self.par_graph_sample('train')\n",
    "                print()\n",
    "\n",
    "            self.node_subgraph = self.subgraphs_remaining_nodes.pop()\n",
    "            self.size_subgraph = len(self.node_subgraph)\n",
    "            adj = sp.csr_matrix(\n",
    "                (\n",
    "                    self.subgraphs_remaining_data.pop(),\n",
    "                    self.subgraphs_remaining_indices.pop(),\n",
    "                    self.subgraphs_remaining_indptr.pop()),\n",
    "                    shape=(self.size_subgraph,self.size_subgraph,\n",
    "                )\n",
    "            )\n",
    "            adj_edge_index = self.subgraphs_remaining_edge_index.pop()\n",
    "            #print(\"{} nodes, {} edges, {} degree\".format(self.node_subgraph.size,adj.size,adj.size/self.node_subgraph.size))\n",
    "            norm_aggr(adj.data, adj_edge_index, self.norm_aggr_train, num_proc=args_global.num_cpu_core)\n",
    "            # adj.data[:] = self.norm_aggr_train[adj_edge_index][:]      # this line is interchangable with the above line\n",
    "            adj = adj_norm(adj, deg=self.deg_train[self.node_subgraph])\n",
    "            adj = _coo_scipy2torch(adj.tocoo())\n",
    "            if self.use_cuda:\n",
    "                adj = adj.cuda()\n",
    "            self.batch_num += 1\n",
    "        norm_loss = self.norm_loss_test if mode in ['val','test', 'valtest'] else self.norm_loss_train\n",
    "        norm_loss = norm_loss[self.node_subgraph]\n",
    "        return self.node_subgraph, adj, norm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    def num_training_batches(self):\n",
    "        return math.ceil(self.node_train.shape[0] / float(self.size_subg_budget))\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.node_train = np.random.permutation(self.node_train)\n",
    "        self.batch_num = -1\n",
    "\n",
    "    def end(self):\n",
    "        return (self.batch_num + 1) * self.size_subg_budget >= self.node_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:zyf]",
   "language": "python",
   "name": "conda-env-zyf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
